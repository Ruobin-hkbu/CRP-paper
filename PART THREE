Part 3: AI Philosophy — From Turing, Licklider to Contemporary AI Ethics and Value Alignment (VA)
  

             Count me out on this one. I never could write poetry.
 
                                          —Alan Turing

 We start the final chapter with two essential figure concerning machine intelligence in the 1950s and 60s, Turing and Licklider: Alan Turing’s dream of AI that could think, decide, and create as autonomous agents, and J.C.R. Licklider’s aspiration for symbiotic collaboration, where machines empower and extend human creativity rather than usurping it. This tension reaches beyond technical boundaries: it echoes a deeper philosophical inquiry about the post-human future. Are machines inexorably becoming more like us, replicating our minds and motivations—or are we, as Donna Haraway’s cyborg metaphor suggests, becoming increasingly entangled with, and shaped by, our technology? 
This chapter traces the intellectual lineage and practical implications of these two paradigms, arguing that the augmentation model offers a more promising path for realizing the full potential of human-AI creative collaboration. We examine not only the historical debates but why and how augmentation, rather than total automation, better serves our ethical, artistic, and existential needs. Moving forward, the discussion turns to contemporary challenges in AI philosophy and ethics, specifically the problem of Value Alignment (VA)—the effort to encode human values, morals, and aesthetic standards into intelligent systems. As AI assumes a greater role in domains from everyday dialogue to artistic creation, the question becomes not only how to align machines with our values, but whose values are and should be privileged and encoded, and how will this effect on AI and on humans. Through this analysis, the chapter addresses critical questions at the heart of AI philosophy: How might we design augmentation that is ethical, generative, and inclusive? What is the future of value alignment in machine-generated art and literature? And, ultimately, what do these inquiries reveal about humanity’s evolving relationship with technology?
3.1 Turing Versus Licklider: Going full automatic or human-AI collaboration 
   In 1950, Alan Turing envisioned a “universal machine” that could, in principle, simulate any formal reasoning process—a concept foundational to modern computing and artificial intelligence. Turing’s “imitation game,” now known as the Turing Test, set the benchmark for machine intelligence: a machine would be considered intelligent if its responses were indistinguishable from a human’s. While this vision inspired the pursuit of fully autonomous, self-improving thinking machines, it also provoked significant philosophical challenges which questioned whether computational mimicry could ever amount to genuine understanding or consciousness. Amid these debates, J.C.R. Licklider offered a contrasting vision of “man-machine symbiosis,” emphasizing a collaborative partnership where human judgment and creativity remain central, and machines serve to augment human capabilities rather than replace them. This shift from the quest for independent artificial general intelligence toward integrated human-computer collaboration continues to shape both the philosophical and practical evolution of artificial intelligence today.
   This session aims to trace the origins of these two contrasting AI paradigms—Turing’s universal machine vision and Licklider’s man-machine symbiosis—and explore their far-reaching philosophical implications. By comparing these modes, the discussion highlights how the emphasis on human agency fundamentally shapes AI development, arguing that Licklider’s approach centers ongoing human decision-making, judgment, evaluation and ethics as essential. 
3.1.1 Evolution of philosophical debates around building a general artificial intelligence over the century 
  In his seminal 1950 paper, “Computing Machinery and Intelligence,” Turing proposed that a digital computer can usually be regarded as consisting of three parts: 1) store 2) executive unit and 3) control; The store is a store of information, the executive unit is the part which carries out various individual operations involved in a calculation, control is the instructions for a sequence of operations to be repeated over and over again until some condition is fulfilled. Hence, a digital computer must be programmed afresh for each new machine which is desired to mimic. But what if there is a computer that can mimic basically any discrete state machines? What if the computers that could, in principle, simulate any process of formal reasoning, just like the way a human would do (Turing, 1950). Turing called it a “universal machine”. 
To test whether if the machine reached the level of human intelligence, Turing introduced the “imitation game,” or Turing Test, as the standards by which machine intelligence would be recognized: if a machine’s responses to questions were indistinguishable from a human’s, it could plausibly be considered intelligent. 
Turing’s vision laid the foundation for the pursuit of full automation—the creation of machines that not only executed prescribed tasks, but could also learn, adapt, and perform creative, open-ended cognition without direct human oversight. Supporters of this paradigm, ranging from early cyberneticists to more recent advocates of artificial general intelligence (AGI), have continued to chase this dream of autonomous, self-improving thinking machines.  
However, Turing’s thesis was not without its critics. Perhaps the most famous challenge came from philosopher John Searle, who formulated the Chinese Room argument in opposed to Turing Test. In Mind, Brains and Program (1980). Searle argued that computational response does not equate to genuine comprehension of the order, analogous to the individual in the room who follows the English instruction, seemingly understands Chinese yet possesses no actual understanding of the language. Searle’s critique highlighted a profound distinction between simulating versus actually having a mind, fueling decades of philosophical debate on whether “artificial general intelligence” is possible, or merely fanciful. Other notable critics have highlighted logical and practical limitations. Hubert Dreyfus argued that human intelligence is rooted in embodied, context-dependent “know-how” that cannot be captured by rule-based computation alone (Dreyfus, 1979). 
The last two decades have witnessed a dramatic shift from rule-based symbolic AI, which dominated early AGI dreams, to statistical and data-driven approaches: machine learning and, more recently, deep learning. Machine learning (ML) emerged as a response to the limitations of rule-based, symbolic artificial intelligence, which struggled with the complexity and ambiguity of real-world data. Instead of programming explicit instructions, ML systems automatically infer patterns and make predictions from large datasets, using statistical and algorithmic methods. Deep learning, advances this paradigm by employing artificial neural networks with multiple layers—so-called “deep” architectures. Inspired by the structure of the human brain, these networks transform raw data into increasingly abstract representations, enabling breakthroughs in tasks such as image and speech recognition, natural language understanding, and complex strategy games (LeCun, Bengio, & Hinton, 2015). The fundamental logic of machine learning and deep learning is thus inductive: models generalize from examples, rather than just follow explicit symbolic rules, and adapt iteratively as more data is processed. 
However, while these systems excel in narrowly defined domains, they often lack the flexible reasoning, transferability. Their intelligence is powerful but domain-specific, heavily dependent on the data and architectures designed by humans. Unlike the hypothetical AGI, these models cannot generalize flexibly; they fail in unfamiliar contexts, struggle with transfer learning, and lack genuine understanding or “common sense” (Marcus & Davis, 2020). Despite their dazzling capabilities, machine learning and deep learning still fall short of the self-reflective, contextual, and embodied cognition that characterizes human thinking. Thus, while Turing’s original dream continues to inspire both utopian and dystopian narratives, the evolution of philosophical and technical debates reveals a persistent gulf: autonomous machine intelligence remains a tantalizing horizon, rather than an imminent reality.
3.1.2 Development of Licklider’s vision: Establishing a post-human future with Artificial Intelligence
   At the beginning of this session, I’d like to revisit John Searle’s Chinese Room argument to highlight a crucial dimension that often goes unexamined: the role of human agency within the thought experiment. Searle conceptualizes the person in the room as a mere conduit, mechanically following precise rules without any comprehension or autonomy—a figure entirely devoid of linguistic knowledge or decision-making power. But in reality, human cognition is not so easily reduced. Even when faced with unfamiliar contexts, individuals inevitably exercise intuition, judgement, and problem-solving, drawing on intellectual habits and sometimes even diverging from strict instructions. Similarly, the Turing Test largely neglects the subtler interplay between human and machine, focusing instead on outputs without regard for the underlying process.  
Both thought experiments, in their quest to isolate “pure” machine intelligence, obscure the inescapable and ongoing involvement of humans in the design, coding, and development of artificial intelligence systems. Each machine, from its foundational algorithms to its training data and parameter adjustments, is fundamentally shaped by human choices, expertise, and sometimes inadvertent biases. In practice machine intelligence is a reflection, extension, or distillation of the human minds that constructed them. Thus, as we examine the pitfalls of automatic machine intelligence, it is crucial to foreground human agency—not as a peripheral detail, but as a core component of the AI story. The creative, intuitive, and directive influence of human participants is woven into every level of AI’s existence, ensuring that, despite the ambition for automation, genuine independence remains elusive. This session aims to bring that agency back into focus, exploring how our intentions, interventions, and ethical commitments continue to interact and shape artificial intelligence.
 The story began with Licklider’s idea of a “Man-Machine Symbiosis”. Unlike Turing who established a fantasy of a general-purposed machine that could move human elements completely from the other side of the equation, Licklider envisioned a deeply integrated cooperation between humans and machines, where the unique strengths of each partner are leveraged to achieve more effective problem-solving and decision-making than either could alone. In this partnership, humans retain control over setting goals, framing questions, and making critical evaluations, while computers handle routine processing and complex computations to support and enhance human insight (Licklider, 1960). The anticipated result is a powerful fusion of human creativity and computer efficiency—enabling both partners to reach new heights of intellectual capability, laying philosophical foundations for all human-machine creative collaborations to date. 
What truly distinguishes Licklider’s vision from the argument over imitation game  is not just his recognition of human agency in the process of machine learning, but the centrality Licklider ascribes to human judgement in guiding technological evolution itself. Rather than attempting to displace human agency with machine autonomy, Licklider’s framework proposes that humans should always remain the arbiters of progress: choosing what advances to embrace, what values to preserve, and how to shape the trajectory of technological development. He imagines humans as the decision-makers who evaluate not only logical correctness and computational optimality, but also weigh those ineffable qualities—empathy, creativity, moral judgment, sentiment—that give human life its warmth and texture. While machines excel in delivering logically perfect and efficient solutions, human choices are often guided by emotion, context, and ethical reflection. These distinctive human factors, often seen as irrational or sub-optimal from a purely computational perspective, are precisely the foundation of what it means to be human and to foster a just and meaningful society.
 This approach fundamentally diverges from the quest for AGI, whether achievable or not, as an isolated, self-sufficient intelligence. Instead, Licklider’s vision anticipates an era of human-computer co-evolution: a future in which machines serve as extensions and augmentations of our own cognitive and creative capacities rather than independent agents. His ideas paved the way for contemporary philosophies like posthumanism, which challenge fixed boundaries between human and machine, suggesting that our identities and agency can, and perhaps will, be transformed by technological mediation (Hayles, 1999). Similarly, Donna Haraway’s influential “Cyborg Manifesto” imagines the cyborg as a hybrid of organism and machine—a figure that blurs the dichotomy of human and technology, embracing a cooperative, intertwined existence (Haraway, 1991), and making “kins”— forging relationships and alliances that stretch beyond traditional lines of species, gender, and kinship—machines (Haraway, 2016).
Thus, the paradigms set forth by Licklider and expanded in posthumanist thought shift our aspirations away from the pursuit of isolated machine intelligence and toward new forms of symbiosis. In this emerging landscape, the future lies not solely in the perfection of computational logic, but in the enrichment of human experience, creativity, and ethical agency through partnership with intelligent systems. The cyborg, as Haraway describes it, is not an alien or adversary, but an inevitable evolution of human-computer collaboration—one that foregrounds human values in the core of technological progress. 

3.2 Why Human-AI cooperation: AI as a “mirror” for humanity
3.2.1 How AI prompts us to reconsider what it means to exist, to create, to judge, and to evaluate. 
   Since the day of its birth, AI has embarked on a relentless pursuit to translate human language, ideas, and conceptions into a sequence of signals consisted of “1”s and “0”s. As it progressively merges with human life, each scrolling of our smartphones, every updating of social media posts, and the sharing of online content results in an unconscious assimilation, dissemination, and consolidation of a certain cognitive pattern. The society forged is what Marcuse (1964) characterized as a “one-dimensional” universe of thought and behavior. Marcuse defends a great refusal to this all-encompassing contemporary mode of thought and endorses “negative thinking,” which encompasses ideas that are dismissed as irrational, mystical, and meaningless, serving as a disruptive force against the dominant one-dimensional positivism. The training and prompting of AI is not merely an endeavor to ‘teach’ machines in understanding humanity, as machines can never authentically grasp human aesthetics, values, and ethics. Rather, it serves to enable humans to recognize the neglected aspects of themselves manifested in every decision-making process through poetry creation with an AI program. The co-creation process therefore transforms what is entirely inhuman back into something human within the human mind. 
   This transformation sits at the heart of our contemporary reckoning with AI—not just as a tool, but as a mirror that forces us to reflect on what it means to exist, to create, to judge, and to evaluate. The very process of interacting with AI, especially in the creative realm, compels us to confront the boundaries that have traditionally separated the rational from the irrational, the mechanical from the organic, the systematized from the spontaneous. Generative AI models can now produce images, stories, and poems that once belonged solely to the domain of human imagination. Yet, as we input prompts and receive outputs, we are reminded that these systems only function by recombining what they have learned from vast corpora—echoes of past human expressions filtered through digital logic. 
The advent of AI also catalyzes moments of resistance and rediscovery. When humans and AI collaborate to craft a poem, for example, the irreducibly human elements—emotion, intuition, ambiguity—inevitably surface in the process. Rather than merely teaching machines to mimic these qualities, we find ourselves prompted to interrogate our own values, preferences, and aesthetic intuitions: Why do we prefer certain imagery? Why does a particular line evoke emotion? What makes a poem “good”, “inspiring”, and “resonating”? These questions demand subjective judgment and self-reflection, qualities that stubbornly resist quantification by algorithms. Thus, engaging with AI in creative processes is not a straightforward act of delegation, but a dialogic re-examination and reaffirmation of human agency. 
In broader terms, the rise of AI in art, literature, and decision-making urges us to reconsider the structures of value, meaning, and authenticity. As algorithmic systems proliferate, it is essential to cultivate ethical standards and collective action that ensure AI elevates, rather than erodes, the richness of human creative and evaluative experience. The interplay between one-dimensional systems and multidimensional imagination, asks us not only what we can create, but what we ought to become in an age of intelligent machines. 

3.3.2 How AI catalyst for new questions in the humanities: What to discard, what to stay 

    
3.3  Further debates on human-AI Collaboration: AI ethics and value alignment (VA) 
There’s a fine line between building an efficient human-machine partnership and enabling the co-development of humans and machines, and it’s this line that puts many computer scientists in a Frankenstein dilemma: are we building machines that promote harmonious coexistence with humans, or are we making more powerful and efficient cooperation but without humanity, sentiments and ethics? 
To address this issue, a philosophical framework we’re increasingly referring to is AI ethics. This session introduces AI ethics and value alignment, outlining how to build trustworthy machines for human–AI symbiosis and practical pathways for collaborative mitigation of harms. AI ethics asks how to ensure AI systems act in accordance with shared human values, while value alignment is the technical and governance task of steering systems toward those values in real-world settings. When alignment is embedded in design, evaluation, and deployment with the norms and needs of affected users in mind, so systems work alongside diverse human interests rather than against them. 
We will therefore focus on “how” to collaborate with AI so that human originality is amplified rather than crowded out, and so that bias, misinformation, and plagiarism are pro-actively minimized through data provenance, consent and attribution practices, and transparent evaluation standards. In creative domains, computational creativity scholarship highlights that creativity is judged by novelty and value, which means our evaluation criteria and workflows strongly shape whether AI outputs enrich or homogenize cultural production. Leading voices in creative AI emphasize the importance of setting responsible objectives, auditing systems for unintended imitation, and establishing shared norms for credit and critique so that machine assistance expands, rather than narrows, the space of human expression. Ultimately, machines do not choose their values—people and organizations do. We will scrutinize the private actors behind these systems, recognizing that current value-setting often reflects the priorities of a concentrated set of developers and firms, and explore institutional designs that re-balance this power. A pluralistic ethics landscape—spanning public, expert, and private actors—requires participatory, deliberative mechanisms to surface contested values and negotiate trade-offs transparently. This session advocates an open-ended, ongoing process of public engagement and evaluation of AI-generated content so that the values encoded in our machines are the collective wisdom of the many, not the dictates of the few. 
  3.3.1 The Things About Creativity: Computational Creativity Versus Human Creativity 
   Computational creativity is a rapidly growing field at the intersection of artificial intelligence, cognitive science, and the arts. Its primary goal is to explore whether computer systems can be genuinely creative, and more broadly, to clarify and expand our understanding of what creativity itself entails. Researchers in this domain aim not merely to automate the creative process, but to investigate the nature of creativity by constructing programs that exhibit creative behavior—be it in art, music, literature, or problem-solving tasks. The theoretical foundation for this work owes much to philosopher Margaret Boden, whose framework distinguishes P-creativity (psychological creativity) from H-creativity (historical creativity) and classifies creativity as combinational, exploratory or transformational, a taxonomy that continues to shape the field. (Boden, 2008)
   However, an important caveat—often overlooked in the excitement over machine-generated art or literature—is that what computers achieve is typically “relative creativity.” In this context, relative creativity means producing outputs that are novel with respect to the system’s inputs, rules, and training data, serving novelty from an engineered or statistical perspective. In contrast, human creativity, at its best, aspires to what might be called “absolute creativity”: the ability to transcend given informational frameworks, to initiate meaning and value in entirely new domains, and to authentically express subjective experience, context, and purpose beyond simple recombination or exploration. Boden’s own work hints at this distinction, emphasizing how truly original human creativity can change how we see ourselves, our history, and our future—something current AI systems cannot genuinely achieve.  
   Reducing all creativity to relative creativity, and thereby suggesting that computers could possess creativity in the same way that humans do, risks impoverishing the very notion of creativity itself. If we consider only what is relatively new within artificial parameters, we risk neglecting the existential, ethical, and imaginative capacities unique to human creators. Therefore, instead of striving to replicate or “surpass” human creativity with machines, the focus should be on using AI to activate, stimulate, and expand human absolute creativity. For example, collaborative tools leveraging computational creativity can help humans break through creative blocks, explore unfamiliar artistic terrains, or extend their own expressive capacities—while always keeping the human at the center of the creative relationship. 
  The rapid advancement of AI-generated content raises pressing questions about creativity itself, particularly in light of societal and ethical concerns. Many computationally creative systems function not as independent originators, but as sophisticated mimics, recombining and re-contextualizing existing data. This leads to a crucial question: is computational creativity merely a replica of information, rather than a creation of genuine truth (Pramanik and Rai, 2022)? AI can convincingly imitate styles, genres, or even the voice of a particular artist, but it does so without authentic subjective experience or intent, raising debates about whether its output should be considered truly original. Loi, Vigano and Plas (2020) concludes that current mainstream AIs are anti-creative because there are moral costs of employing them in human endeavors. The study argues for the fostering of creativity-enabling AI. This type of AI is ethically valuable because it supports long-term human well-being and advances individual and societal flourishing, even though this may sometimes require balancing creativity with other AI ethics priorities such as explainability and accuracy.
   A related issue concerns the democratizing potential and the possible devaluation of artistic and literary work. On one hand, AI offers a bridge between professional writers and ordinary people, dramatically lowering barriers to creative participation by providing powerful tools accessible to all levels of expertise. Ideas, motifs, and techniques that were once the preserve of trained specialists can now be explored by anyone with access to user-friendly software Yet, as this ubiquity spreads, the singularity and authenticity of artistic expression can be diminished (Jiang et al, 2023). When the unique signatures of artists or writers are easily imitated, and creative works become endlessly reproducible, a risk emerges: the value of creative output as something unique and genuine may be undermined. Societies and markets will need to reconsider how creative works are valued and protected, and what constitutes authorship and originality in an era of machine co-creation. 
   Ultimately, while AI and computational creativity are reshaping cultural landscapes, human creativity remains irreplaceably precious. The most valuable art, literature, and scientific breakthroughs arise not from the statistical combinations of past data, but from the unpredictable, meaningful, and courageous leaps that only humans—conscious of their histories, hopes, and responsibilities—can make. Rather than seeing computational creativity as a parallel to human creativity, we should approach AI as a catalyst for deeper, more profound expressions of human creativity, one that inspires us to clarify and cherish what truly makes creative acts significant and enduring.
3.3.2 Contemporary concerns about AI: Who’s responsible for bias, inequality and cultural stereotype
From automating everyday tasks to generating art, music, and even news, AI is shaping the way we live and interact. However, as this technology advances, so do the concerns surrounding its use. Weidigner et al. (2021) concludes sixth hazards concerning the use of LLMs can perpetuate stereotypes, enable unfair discrimination, and reinforce exclusionary social norms, highlighting significant ethical and social risks associated with their deploymentand social biases. 
AI is not inherently biased, nor is it a force that inevitably leads to social harm or the erosion of artistic value. Rather, the real drivers of these problems are the people and institutions behind AI—the programmers, the large tech corporations, and, more broadly, the powerful private actors who design, train, and profit from these systems. When we see an AI tool that produces biased outputs, it is often because it has been trained on data that reflects existing societal prejudices. The training data is curated—whether intentionally or inadvertently—by humans. The algorithms are written and tweaked by teams of engineers, often working under pressure to meet corporate goals or to serve specific markets. The decisions about what data to use, what outcomes to prioritize, and even what ethical guidelines to follow are made not by the AI, but by people and organizations with their own interests and limitations. Checketts (2024) argues that the real concern with AI and poverty lies in how technological approaches reinforce societal assumptions about dignity and person-hood, often marginalizing those experiencing poverty instead of empowering them. The book calls for critical reflection on the underlying values embedded in AI systems and urges policymakers, technologists, and communities to center human dignity, inclusiveness, and transparency when designing and implementing AI solutions.
To address these challenges, we must look beyond the surface of the technology itself. Blaming AI as a monolithic threat distracts us from the real issue: the lack of transparency, accountability, and inclusiveness in how AI is developed and used. Instead of fearing AI, we should demand greater openness about how these systems work, who controls them, and whose interests they serve. Li (2020) articulates that the choices we make today will determine whether AI serves as a tool for empowerment and inclusion or inadvertently deepens divisions and exclusions. AI revolution is a call to collective action and ethical reflection.It urges us to a methodology that promotes transparency, inclusivity, and respect for humanity. AI should not be left unchecked of corporation or technocrats, but must be guided by a more democratic, collective, inclusive call. 
  
3.3.3 Questions of AI value alignment (VA): How to VA? Whose values? How to evaluate the result of VA?
AI value alignment, commonly referred to as VA, is the idea of enhancing artificial intelligence by deliberately incorporating human values, judgment, and ethics directly into computer programs, thereby making AI more accountable and aligned with the priorities of its designers and users. 
The challenge here is not simply one of technical improvement, it’s a complex task involving technical, ethical and societal commitment. As far as this study concerns, the challenge of VA is three-folded. The first and most foundational problem revolves around subjectivity. When programming AI systems with human values, two key questions emerge: Are humans guiding and assisting AI towards ethical behavior, or is AI, increasingly sophisticated, affecting human value systems and practices through its own processes? Therefore,  Gabriel and Ghazavi(2022) calls for multidisciplinary collaboration, robust auditing mechanisms, and continuous reflections to ensure that AI technologies remain beneficial and aligned with evolving human priorities. AI value alignment, according to the study, must be a intertwined, interactive and evolving system that consists continuous human effort, supervision, and reflection. 
The second key issue is diversity. Human societies are pluralistic, containing vast arrays of perspectives, cultures, and beliefs. If we program AI to adhere to a consensus-based rule set, how do we ensure it does not inadvertently exclude minority views or stifle valuable dissent? There is also the risk of homogenization: aiming for consensus can lead to generic outputs, which may compromise the richness and diversity of AI-generated content. This is particularly concerning in domains such as art, innovation, and public discourse, where diversity of thought and creative tension are crucial. In their study, Weidinger et al. (2023) identify three main approaches to value alignment, all of which aim to secure a certain type of human consensus regarding what is “acceptable” or “desirable” behavior from AI. These approaches are known as consensus-oriented frameworks: they strive to distill shared values—whether derived from majoritarian agreement, cross-cultural principles, or expert deliberation—into an actionable set of guidelines that AI systems can follow. 
The final question has to do with the evaluation of VA as a result. Without a way to measure value alignment, we are unable to determine whether one AI system is more closely aligned with human morality than another. Peterson and Gärdenfors (2024) involves developing two quantitative measures based on conceptual spaces, representing human values and norms as geometric regions in multidimensional similarity spaces, allowing precise estimation of how closely an AI system’s outputs align with those values. The approach avoids the need to predefine utility values for moral violations, providing a flexible and principled framework for measuring value alignment in diverse AI applications. However, this method is only applicable when the indicators can be quantified. If some values, such as aesthetics, poetics, human experience, emotion, resonance, etc., are abstract and difficult to quantify, how can they be measured? Therefore, this study believes that VA still requires the joint and cyclical participation of both algorithm and human evaluators/measurers.
